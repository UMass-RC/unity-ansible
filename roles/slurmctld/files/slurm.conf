# slurm.conf file generated by configurator.html.
# Put this file on all nodes of your cluster.
# See the slurm.conf man page for more information.
#
SlurmctldHost=slurmctld1
SlurmctldHost=slurmctld2
SlurmctldParameters=enable_configless
#SlurmctldHost=
#DisableRootJobs=NO 
EnforcePartLimits=ALL
Epilog=/etc/slurm/unity-epilog.sh
#EpilogSlurmctld= 
FirstJobId=1332880
#MaxJobId=999999 
GresTypes=gpu
#GroupUpdateForce=0 
#GroupUpdateTime=600 
#JobFileAppend=0 
JobRequeue=1 
#JobSubmitPlugins=1 
#KillOnBadExit=0 
#LaunchType=launch/slurm 
#Licenses=foo*4,bar 
MailProg=/opt/slurm-mail/bin/slurm-spool-mail.py
#MaxJobCount=5000 
#MaxStepCount=40000 
#MaxTasksPerNode=128 
MpiDefault=pmi2
#LaunchParameters=mpir_use_nodeaddr
#MpiParams=ports=#-# 
#PluginDir= 
#PlugStackConfig= 
#PrivateData=jobs 
ProctrackType=proctrack/cgroup
Prolog=/etc/slurm/unity-prolog.sh
PrologFlags=x11 
#PrologSlurmctld= 
#PropagatePrioProcess=0 
#PropagateResourceLimits= 
#PropagateResourceLimitsExcept= 
#RebootProgram= 
ReturnToService=2
#SallocDefaultCommand= 
SlurmctldPidFile=/var/run/slurmctld.pid
SlurmctldPort=6817
SlurmdPidFile=/var/run/slurmd.pid
SlurmdPort=6818
SlurmdSpoolDir=/var/spool/slurm/slurmd
SlurmUser=slurm
#SlurmdUser=root 
#SrunEpilog=
#SrunProlog=
StateSaveLocation=/var/spool/slurm
SwitchType=switch/none
TaskEpilog=/etc/slurm/unity-taskepilog.sh
TaskPlugin=task/cgroup
TaskPluginParam=Sched
TaskProlog=/etc/slurm/unity-taskprolog.sh
#TopologyPlugin=topology/tree 
#TmpFS=/tmp 
#TrackWCKey=no 
#TreeWidth= 
#UnkillableStepProgram= 
#UsePAM=0 
# 
# 
# TIMERS 
#BatchStartTimeout=10 
#CompleteWait=0 
#EpilogMsgTime=2000 
#GetEnvTimeout=2 
#HealthCheckInterval=0 
#HealthCheckProgram= 
InactiveLimit=0
KillWait=30
#MessageTimeout=10 
#ResvOverRun=0 
MinJobAge=300
#OverTimeLimit=0 
SlurmctldTimeout=120
SlurmdTimeout=300
UnkillableStepTimeout=300
#VSizeFactor=0 
Waittime=0
# 
# 
# SCHEDULING 
DefMemPerCPU=1024
#FastSchedule=1
#MaxMemPerCPU=0 
#SchedulerTimeSlice=30 
SchedulerType=sched/backfill
SchedulerParameters=bf_continue,bf_busy_nodes
SelectType=select/cons_tres
SelectTypeParameters=CR_Core_Memory
# 
# 
# JOB PRIORITY 
#PriorityFlags= 
PriorityType=priority/multifactor
#PriorityType=priority/basic
#PriorityDecayHalfLife= 
#PriorityCalcPeriod= 
#PriorityFavorSmall= 
#PriorityMaxAge= 
#PriorityUsageResetPeriod= 
#PriorityWeightAge= 
#PriorityWeightFairshare= 
#PriorityWeightJobSize= 
#PriorityWeightPartition= 
#PriorityWeightQOS= 
# 
# 
# LOGGING AND ACCOUNTING 
AccountingStorageEnforce=associations,qos,limits
AccountingStorageHost=slurmdbd
#AccountingStorageTRES=gres/gpu
AccountingStorageTRES=gres/gpu,gres/gpu:v100,gres/gpu:2080,gres/gpu:2080ti,gres/gpu:titanx
#AccountingStorageLoc=
#AccountingStoragePass=
#AccountingStoragePort=
AccountingStorageType=accounting_storage/slurmdbd
#AccountingStorageUser=
AccountingStoreJobComment=YES
ClusterName=unity
#DebugFlags=
#JobCompHost=
#JobCompLoc=elastic.maas:9200/unity
#JobCompPass=
#JobCompPort=
#JobCompLoc=/tmp/jobcomp.txt
#JobCompType=jobcomp/filetxt
JobCompType=jobcomp/none
#JobCompUser=
#JobContainerType=job_container/none 
JobAcctGatherFrequency=30
JobAcctGatherType=jobacct_gather/linux
SlurmctldDebug=error
SlurmctldLogFile=/var/log/slurm/slurmctld.log
SlurmdDebug=error
SlurmdLogFile=/var/log/slurm/slurmd.log
#SlurmSchedLogFile= 
#SlurmSchedLogLevel= 
# 
# 
# POWER SAVE SUPPORT FOR IDLE NODES (optional) 
#SuspendProgram= 
#ResumeProgram= 
#SuspendTimeout= 
#ResumeTimeout= 
#ResumeRate= 
#SuspendExcNodes= 
#SuspendExcParts= 
#SuspendRate= 
#SuspendTime= 
# 
PreemptType=preempt/partition_prio
PreemptMode=REQUEUE

PriorityWeightAge=100
PriorityWeightFairshare=500
PriorityWeightJobSize=100
PriorityMaxAge=1-0
PriorityUsageResetPeriod=MONTHLY

### Non-Compute ###
NodeName=gypsum-eguide CoresPerSocket=6 RealMemory=192000 Sockets=2 ThreadsPerCore=1

### NODES ###
NodeName=cpu001,cpu[005-008] Features=len_sd530_2018,intel Sockets=2 CoresPerSocket=12 ThreadsPerCore=2 RealMemory=192971
NodeName=cpu[002-004] Features=len_sd530_2018,intel Sockets=2 CoresPerSocket=12 ThreadsPerCore=2 RealMemory=386286
NodeName=gpu[001-002] Features=len_sr650_2018,intel Sockets=2 CoresPerSocket=8 ThreadsPerCore=2 RealMemory=192833 Gres=gpu:v100:2
NodeName=gpu[003-004] Features=dell_r740_2020,intel Sockets=2 CoresPerSocket=18 ThreadsPerCore=2 RealMemory=191914 Gres=gpu:v100:2
NodeName=cpu[009-021] Features=dell_r640_2020,intel Sockets=2 CoresPerSocket=20 ThreadsPerCore=2 RealMemory=192040
NodeName=ceewater-cpu[001-007] Features=cee_len_sr635_2020,amd Sockets=1 CoresPerSocket=24 ThreadsPerCore=2 RealMemory=128749
NodeName=astroth-gpu[001-003] Features=astro_asrock_x399_2020,intel Sockets=1 CoresPerSocket=8 ThreadsPerCore=2 RealMemory=32033 Gres=gpu:2080:2
NodeName=ials-gpu[001-033],node92 Features=ials_gigabyte_gpu_2020,intel Sockets=2 CoresPerSocket=12 ThreadsPerCore=2 RealMemory=191914 Gres=gpu:2080ti:8
NodeName=astroth-cpu[001-008] Features=astro_smicro_sbi4429pt2n_2021,intel CPUs=32 Boards=1 SocketsPerBoard=2 CoresPerSocket=8 ThreadsPerCore=2 RealMemory=176724
NodeName=zhoulin-cpu[001-006] Features=zhoulin_sr645_cpu_2021,amd Boards=1 SocketsPerBoard=2 CoresPerSocket=64 ThreadsPerCore=2 RealMemory=515863
NodeName=toltec-cpu[001-007] Features=toltec_r640_cpu_2021,intel Boards=1 SocketsPerBoard=2 CoresPerSocket=16 ThreadsPerCore=1 RealMemory=385588
NodeName=ece-gpu[001-002] Features=ece_sr670_gpu_2021,intel Boards=1 SocketsPerBoard=2 CoresPerSocket=16 ThreadsPerCore=2 RealMemory=386510 Gres=gpu:28
NodeName=gypsum-gpu[001-025] CPUs=24 Boards=1 SocketsPerBoard=2 CoresPerSocket=6 ThreadsPerCore=2 RealMemory=257849 Gres=gpu:m40:4
NodeName=gypsum-gpu[026-100] CPUs=24 Boards=1 SocketsPerBoard=2 CoresPerSocket=6 ThreadsPerCore=2 RealMemory=257849 Gres=gpu:titanx:4
NodeName=gypsum-gpu[104-156] CoresPerSocket=12 Sockets=2 ThreadsPerCore=2 Gres=gpu:8
NodeName=gypsum-gpu[157-181] CoresPerSocket=12 Sockets=2 ThreadsPerCore=2 Gres=gpu:8
NodeName=gypsum-gpu[182-189] CoresPerSocket=12 Sockets=2 ThreadsPerCore=2 Gres=gpu:8
NodeName=gypsum-gpu[190-192] CoresPerSocket=12 Sockets=2 ThreadsPerCore=2 Gres=gpu:8

### PARTITIONS ###
PartitionName=cpu Nodes=cpu[001-021] PriorityTier=1 Default=YES DefaultTime=04:00:00 MaxTime=1-0
PartitionName=cpu-long Nodes=cpu[001-021] PriorityTier=1 DefaultTime=3-0 MaxTime=14-0
PartitionName=gpu Nodes=gpu[001-004],ials-gpu[001-033] PriorityTier=2 DefaultTime=04:00:00 MaxTime=1-0
PartitionName=gpu-long Nodes=gpu[001-004],ials-gpu[001-033] PriorityTier=2 DefaultTime=7-0 MaxTime=14-0
PartitionName=cpu-preempt Nodes=gpu[001-004],ials-gpu[001-033],ceewater-cpu[001-007],astroth-cpu[001-008],zhoulin-cpu[001-006],toltec-cpu[001-007] PriorityTier=1 DefaultTime=04:00:00 MaxTime=1-0 QOS=priority
PartitionName=gpu-preempt Nodes=astroth-gpu[001-003],gypsum-gpu[001-100],gypsum-gpu[104-192] PriorityTier=1 DefaultTime=04:00:00 MaxTime=1-0
PartitionName=cee_water_cjgleason Nodes=ceewater-cpu[001-007] PriorityTier=20 AllowAccounts=pi_cjgleason_umass_edu QOS=priority GraceTime=7200
PartitionName=cee_water_casey Nodes=ceewater-cpu[001-007] PriorityTier=10 AllowAccounts=pi_casey_umass_edu QOS=priority GraceTime=7200
PartitionName=cee_water_kandread Nodes=ceewater-cpu[001-007] PriorityTier=10 AllowAccounts=pi_kandread_umass_edu QOS=priority GraceTime=7200
PartitionName=ials_gpu Nodes=ials-gpu[001-033] PriorityTier=2 AllowAccounts=pi_jianhanc_umass_edu QOS=priority
PartitionName=astro_th Nodes=astroth-cpu[001-008] PriorityTier=10 AllowAccounts=pi_mdw_umass_edu,pi_pkatz_umass_edu QOS=priority GraceTime=7200
PartitionName=astro_th_gpu Nodes=astroth-gpu[001-003] PriorityTier=10 AllowAccounts=pi_mdw_umass_edu,pi_pkatz_umass_edu QOS=priority GraceTime=7200
PartitionName=zhoulin_cpu Nodes=zhoulin-cpu[001-006] PriorityTier=10 AllowAccounts=pi_zhoulin_umass_edu QOS=priority GraceTime=7200
PartitionName=toltec_cpu Nodes=toltec-cpu[001-007] PriorityTier=10 AllowAccounts=pi_gwilson_umass_edu QOS=priority GraceTime=7200
PartitionName=ece Nodes=ece-gpu[001-002] PriorityTier=10 AllowAccounts=pi_jgriffin_umass_edu QOS=priority

# Gypsum
PartitionName=gypsum-m40-phd Nodes=gypsum[1-25] MaxTime=7-0 TRESBillingWeights=gres/gpu=1.0 PriorityTier=10 AllowGroups=gypsum_access_phd QOS=gypsum-phd-m40
PartitionName=gypsum-m40-ms Nodes=gypsum[1-25] MaxTime=7-0 TRESBillingWeights=gres/gpu=1.0 PriorityTier=10 AllowGroups=gypsum_access_ms QOS=gypsum-ms-m40
PartitionName=gypsum-m40-course Nodes=gypsum[1-25] MaxTime=7-0 TRESBillingWeights=gres/gpu=1.0 PriorityTier=10 AllowGroups=gypsum_access_course QOS=gypsum-course-m40
PartitionName=gypsum-titanx-phd Nodes=gypsum[26-99] MaxTime=7-0 TRESBillingWeights=gres/gpu=1.0 PriorityTier=10 AllowGroups=gypsum_access_phd QOS=gypsum-phd-titanx
PartitionName=gypsum-titanx-ms Nodes=gypsum[26-99] MaxTime=7-0 TRESBillingWeights=gres/gpu=1.0 PriorityTier=10 AllowGroups=gypsum_access_ms QOS=gypsum-ms-titanx
PartitionName=gypsum-titanx-course Nodes=gypsum[26-99] MaxTime=7-0 TRESBillingWeights=gres/gpu=1.0 PriorityTier=10 AllowGroups=gypsum_access_course QOS=gypsum-course-titanx
#PartitionName=gypsum-1080ti Nodes=gypsum[104-156] MaxTime=7-0 TRESBillingWeights=gres/gpu=1.0 PriorityTier=10 AllowGroups=gypsum_access QOS=priority
#PartitionName=gypsum-2080ti Nodes=gypsum[157-181],gypsum[190-192] MaxTime=7-0 TRESBillingWeights=gres/gpu=1.0 AllowGroups=gypsum_access QOS=priority